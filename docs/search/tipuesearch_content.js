var tipuesearch = {"pages":[{"title":"Changelog","text":"  All notable changes to this project will be documented in this file.  The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.  [Unreleased]   lorem   [N.N.N] - YYYY-MM-DD  Added   lorem   Changed   lorem   Deprecated   lorem   Removed   lorem   Fixed   lorem   Security   lorem  ","tags":"","url":"CHANGELOG.html"},{"title":"Website Backup","text":"    Summary  A script to perform routine backups of critical website files and database.  Highlights include:   Cloud backup to AWS S3. Automatically deletes older backups. YAML based configuration. Easily cherry-pick files or folders to include in backup.   Visit https:\/\/aklump.github.io\/website_backup for full documentation.  Quick Start   Install in your repository root using cloudy pm-install aklump\/website_backup Open bin\/config\/website_backup.yml and modify as needed. Open bin\/config\/website_backup.local.yml and ...; be sure to ignore this file in SCM as it contains your AWS credentials. Create a cron job that executes bin\/website_backup at the desired interval, e.g.  0 1 * * * \/var\/www\/mywebsite.org\/app\/bin\/website_backup bu  2&gt;&amp;1 | mail -s \"FOO backup\" me@developer.com  In some cases you must override $TMPDIR, something like this (replacing the path to the temporary directory as appropriates for your situation):  0 1 * * * export TMPDIR=\"\/home\/foo\/tmp\"; \/var\/www\/mywebsite.org\/app\/bin\/website_backup bu  2&gt;&amp;1 | mail -s \"FOO backup\" me@developer.com    Requirements  You must have Cloudy installed on your system to install this package.  Installation  The installation script above will generate the following structure where . is your repository root.  . \u251c\u2500\u2500 bin \u2502\u00a0\u00a0 \u251c\u2500\u2500 website_backup -&gt; ..\/opt\/website_backup\/website_backup.sh \u2502\u00a0\u00a0 \u2514\u2500\u2500 config \u2502\u00a0\u00a0     \u251c\u2500\u2500 website_backup.yml \u2502\u00a0\u00a0     \u2514\u2500\u2500 website_backup.local.yml \u251c\u2500\u2500 opt \u2502   \u251c\u2500\u2500 cloudy \u2502   \u2514\u2500\u2500 aklump \u2502       \u2514\u2500\u2500 website_backup \u2514\u2500\u2500 {public web root}   To Update   Update to the latest version from your repo root: cloudy pm-update aklump\/website_backup   Configuration Files       Filename   Description   VCS       website_backup.yml   Configuration shared across all server environments: prod, staging, dev   yes     website_backup.local.yml   Configuration overrides for a single environment; not version controlled.   no     Usage   To see all commands use .\/bin\/website_backup   Contributing  If you find this project useful... please consider making a donation. ","tags":"","url":"README.html"},{"title":"Cache Tables by Framework","text":"  Use these as a starting point for configuration. You may want to check if all the tables listed are actually in your database before listing them in your config, e.g. accesslog is Drupal 7 only, I believe.  Drupal  database:   cache_tables:     - accesslog     - batch     - cache     - cache_*     - captcha_sessions     - field_deleted_data_*     - flood     - honeypot_user     - node_access     - old_*     - queue     - search_index     - sessions     - watchdog    [https:\/\/www.drupal.org\/docs\/7\/modules\/backup-and-migrate\/recommendations-for-making-backups-more-reliable](Recommendations for making backups more reliable) https:\/\/drupal.stackexchange.com\/a\/171884\/26195   Wordpress  @todo ","tags":"","url":"cache_tables.html"},{"title":"Database Handlers","text":"   Given an id such as foo, you must create plugins\/db\/foo.sh. It should define the configuration as needed, consider prefixing with plugin name if appropriate. The configuration must be shown in the header comment, e.g.  # # @file # Handles the database export using foo # #   database: #    handler: foo #    foo: bar #  It must provide a backup of the database using whatever config it wants and place it in $path_to_stage. The script runs with $APP_ROOT as the cwd. It must use $database_dumpfile as the output filename. For configuration errors use exit_with_failure* For execution errors use fail_because and exit non-zero.  You may also use write_log_error as appropriate. See existing plugins as a starting point.  ","tags":"","url":"db_handler.html"},{"title":"Lando Integration","text":"   Use the mysqldump plugin. Create tooling in lando.yml Use lando website-backup to execute.   In Detail  To integration with Lando you need to create some tooling scripts in lando.yml.  Here is an example.      tooling:       website-backup:         service: appserver         description: Send a backup of the db and user files to S3.         cmd: \"cd \/app &amp;&amp; .\/bin\/website_backup backup\"   In this second example we are going to store backups locally in the purgeable folder and ignore the files.  This should be connected to Loft Dev for safety backups.      tooling:       website-backup-local-db:         service: appserver         description: Save a local db-only snapshot.         cmd: \"cd \/app &amp;&amp; .\/bin\/website_backup backup -d --local=\/app\/private\/default\/db\/purgeable\"   Then to run the backups you need only do:      lando website-backup     lando website-backup-local-db  ","tags":"","url":"lando.html"},{"title":"Setting Up S3","text":"   Create an S3 account and login. Create a bucket to hold the backups.  Use server-side encryption. Create an access policy, e.g., backup--BUCKET using something like the following, replacing BUCKET with the bucket name.  {     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Effect\": \"Allow\",             \"Action\": [                 \"s3:PutObjectAcl\",                 \"s3:DeleteObject\",                 \"s3:GetObject\",                 \"s3:PutObject\"             ],             \"Resource\": [                 \"arn:aws:s3:::BUCKET\/*\"             ]         },         {             \"Effect\": \"Allow\",             \"Action\": [                 \"s3:ListBucket\"             ],             \"Resource\": [                 \"arn:aws:s3:::BUCKET\"             ]         }     ] }  Create an AWS dedicated user with Programmatic access only to run the script. Add that policy to the user.    ","tags":"","url":"s3.html"},{"title":"Search Results","text":" ","tags":"","url":"search--results.html"},{"title":"Troubleshooting","text":"  Backups Fail Due to Memory Issues  You may need to define a temporary directory other than the default if you run out of room during backup.  Add something like the following to .bash_profile.  You should only do this if you are not able to make a backup due to an error resembling, \"No space left on device\".  This seems to happen when a server's default tmp directory has insufficient space.  export TMPDIR=\"\/home\/foo\/tmp\"   You will also have to provide the override in your crontab.  Enable Logging to Find Errors   Open bin\/website_backup and uncomment the logging line, e.g.,  LOGFILE=\"website_backup.core.log\" Set the path appropriate to your system. Try a backup and then review the log file.  ","tags":"","url":"troubleshooting.html"}]};
